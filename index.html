<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Moderador Comentarios IA</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="container">
    <h1>Noticias y Comentarios</h1>
    <form id="formComentario">
      <textarea id="inputComentario" rows="4" placeholder="Escribe tu comentario..."></textarea>
      <p id="mensajeError" class="error"></p>
      <button type="submit">Enviar</button>
    </form>
    <div id="listaComentarios"></div>
  </div>

  <script>
    const form = document.getElementById("formComentario");
    const input = document.getElementById("inputComentario");
    const mensajeError = document.getElementById("mensajeError");
    const lista = document.getElementById("listaComentarios");

    const palabrasBloqueadas = ["bruto", "feo", "webon"];
    const palabraContexto = "negro";
    const contextoNeutral = ["televisor", "ventilador", "auto", "camisa", "color", "pantalón", "zapato", "carro", "celular"];

    function filtroManual(texto) {
      const textoLower = texto.toLowerCase();
      for (const palabra of palabrasBloqueadas) {
        if (textoLower.includes(palabra)) {
          return true;
        }
      }
      if (textoLower.includes(palabraContexto)) {
        const tieneContextoNeutral = contextoNeutral.some(ctx => textoLower.includes(ctx));
        if (!tieneContextoNeutral) return true;
      }
      return false;
    }

    const TOKEN = "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";
    const MODEL_URL = "https://api-inference.huggingface.co/models/unitary/toxic-bert";
    const UMBRAL_TOXICO = 0.35;

    async function analizarOracion(oracion) {
      const res = await fetch(MODEL_URL, {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${TOKEN}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({ inputs: oracion })
      });

      const resultado = await res.json();
      if (resultado.error) {
        console.error("Error del modelo:", resultado.error);
        return false;
      }

      const etiquetasToxicas = [
        "toxic", "severe_toxicity", "obscene", "threat", "insult", "identity_hate", "sexual_explicit"
      ];

      return resultado[0].some(etiqueta =>
        etiquetasToxicas.includes(etiqueta.label.toLowerCase()) && etiqueta.score > UMBRAL_TOXICO
      );
    }

    async function esToxico(texto) {
      if (filtroManual(texto)) return true;
      const oraciones = texto.split(/[.!?]+/).map(s => s.trim()).filter(s => s.length > 0);
      for (const oracion of oraciones) {
        const toxico = await analizarOracion(oracion);
        if (toxico) return true;
      }
      return false;
    }

    form.addEventListener("submit", async (e) => {
      e.preventDefault();
      mensajeError.textContent = "";
      const texto = input.value.trim();

      if (texto === "") {
        mensajeError.textContent = "El comentario no puede estar vacío.";
        return;
      }

      const toxic = await esToxico(texto);
      if (toxic) {
        mensajeError.textContent = "Tu comentario contiene lenguaje ofensivo y ha sido bloqueado.";
        return;
      }

      const div = document.createElement("div");
      div.classList.add("comentario");
      div.textContent = texto;
      lista.appendChild(div);
      input.value = "";
    });
  </script>
</body>
</html>
